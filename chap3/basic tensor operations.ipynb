{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fe025fe-b58b-45f4-be8d-5bc0ba7a71f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2510fd7-b5e0-4c86-b81e-38d76104c05d",
   "metadata": {},
   "source": [
    "can create a tensor full of ones with `tensor.ones`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "583973c7-023f-4991-97f1-e4abc2f8005f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58e58d2-cae3-49ea-bc67-ad2fd424540a",
   "metadata": {},
   "source": [
    "can give multiple dimennsions, doesn't need to be a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9afe2ef-7621-4099-a6d1-81e3aa9d2e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.ones(3,3)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc6ec2d-e28f-45e6-9611-d0c92465e2ce",
   "metadata": {},
   "source": [
    "You can use `dtype` to get the type (`float32` by default) and `shape` to get the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ad2a6da-8a02-4d3a-b1f8-c02bafd3cfa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.Size([3, 3]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.dtype, b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fd5f36-a4ec-44a2-9889-a87cc1f67cc8",
   "metadata": {},
   "source": [
    "you can use index by multiple axes, just like numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9152125-b0bd-49c1-af0f-54331a23dad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.0000,  1.0000],\n",
       "        [ 1.0000,  1.0000, 99.1000],\n",
       "        [ 1.0000,  1.0000,  1.0000]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[1,2] = 99.1\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfdfeb9-dc1a-4661-8957-93bfc0b50a0f",
   "metadata": {},
   "source": [
    "and you can slice on them the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02cb603f-7d6b-4828-9c8f-736437baac05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000, 99.1000],\n",
       "        [ 1.0000,  1.0000]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[1:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c6658f-0399-4d88-a161-c559f2bfe42e",
   "metadata": {},
   "source": [
    "Imagine we have an RGB image (here represented by a 5x5 array of RGB triples) and we want to convert it to grayscale. We might weight the conversion as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bd197ad-6332-4b7e-b0f8-5c2c67a02070",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = torch.randn(3, 5, 5) # shape [channels, rows, columns]\n",
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddccd48-c353-4f43-b53e-fea22d05b53f",
   "metadata": {},
   "source": [
    "let's set a batch size of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "707b36e6-6c09-414d-b30f-e026a76117c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_t = torch.randn(2, 3, 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b8e4bf-ea2d-4f9e-8f83-9613e7c3d641",
   "metadata": {},
   "source": [
    "so our RGB channels are now in column zero in img_t and column one in batch_t - but they are always third from last so we can use -3 to refer to them. Here's the grayscale conversion without using our weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b82deaf-4ec0-4a70-a89f-39b03ce173ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive = img_t.mean(-3)\n",
    "batch_gray_naive = batch_t.mean(-3)\n",
    "img_gray_naive.shape, batch_gray_naive.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1828958-31d1-41b8-b790-617b8d53fbd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "now we can try it with the weights. First, we can expand the weights size to match the image size.\n",
    "- It's not clear to me why the author used -1 instead of 1; the results are identical\n",
    "- unsqueeze_ modifies the tensor in place, while unsqueeze returns a new view on the same data but doesn't modify the shape of the underlying tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdc1f350-a607-407d-a56d-7b1d26e84d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.2126]],\n",
       " \n",
       "         [[0.7152]],\n",
       " \n",
       "         [[0.0722]]]),\n",
       " torch.Size([3, 1, 1]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze_(-1)\n",
    "unsqueezed_weights, unsqueezed_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6a5aaf-c327-42c5-a04c-8c590919945f",
   "metadata": {},
   "source": [
    "Torch has a function `einsum`, adapted from NumPy, which specifies an indexing [mini-language](https://rockt.github.io/2018/04/30/einsum). Broadcasting is done using three dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1214d4cd-b2bb-4f5c-b0eb-001e7497074b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted_ein = torch.einsum('...chw,c->...hw', img_t, weights)\n",
    "batch_gray_weighted_ein = torch.einsum('...chw,c->...hw', batch_t, weights)\n",
    "img_gray_weighted_ein.shape, batch_gray_weighted_ein.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8b9996-6731-47e7-a6d9-5352ae065f7c",
   "metadata": {},
   "source": [
    "I don't understand what's going on there at all, and [it has been noticed by others](http://nlp.seas.harvard.edu/NamedTensor).\n",
    "\n",
    "To try and improve the situation, torch added _named tensors_.\n",
    "\n",
    "[tutorial](https://pytorch.org/tutorials/intermediate/named_tensor_tutorial.html), [docs](https://pytorch.org/docs/stable/named_tensor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "354dc1b9-457f-4cef-b83b-2361ae5f5cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cj/3fpctnd52vv37y9gpdgg7c200000gn/T/ipykernel_61057/2371314847.py:1: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/c10/core/TensorImpl.h:1761.)\n",
      "  weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels'])\n",
    "weights_named"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4b8a7d-60a4-4bcf-982b-62c9f9f92bca",
   "metadata": {},
   "source": [
    "- When we already have a tensor and want to add names, we can use `refine_names`\n",
    "- `...` allows you to elide any number of dimensions so you can name the ones you want\n",
    "- `rename` allows you to overwrite or drop names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9f492dd-2f66-4dc9-a294-651dff36bb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('channels', 'rows', 'columns'), (None, 'channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_named = img_t.refine_names('channels', 'rows', 'columns')\n",
    "batch_named = batch_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "img_named.names, batch_named.names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b6bb44-b8db-4b02-92e3-0fa25fe464cb",
   "metadata": {},
   "source": [
    "In addition to dimension checks, torch will now check names when we do operations.\n",
    "\n",
    "It does not automatically align dimensions, so we need to do so explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e85bd74f-4aed-488b-9596-47ff4bf41876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.2126]],\n",
       " \n",
       "         [[0.7152]],\n",
       " \n",
       "         [[0.0722]]], names=('channels', 'rows', 'columns')),\n",
       " tensor([0.2126, 0.7152, 0.0722], names=('channels',)))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_aligned = weights_named.align_as(img_named)\n",
    "weights_aligned, weights_named"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33598ab7-d038-4ce1-83d3-6e180cb30fe1",
   "metadata": {},
   "source": [
    "Functions accepting dimensions, like sum, accept names instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "192b9ee7-56b2-4a03-b1bd-c2baef0c993d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), ('rows', 'columns'))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_named = (img_named * weights_aligned).sum('channels')\n",
    "gray_named.shape, gray_named.names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dea3c3-1cfa-454f-bb12-818965742421",
   "metadata": {},
   "source": [
    "combining dimensions with different names yields an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0b8bc7a-aaad-4272-9b4d-418b644072c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cj/3fpctnd52vv37y9gpdgg7c200000gn/T/ipykernel_61057/9594579.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgray_named\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_named\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights_named\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'channels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match."
     ]
    }
   ],
   "source": [
    "gray_named = (img_named[..., :3] * weights_named).sum('channels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d4fc6d-6143-4d48-9602-fbf668154708",
   "metadata": {},
   "source": [
    "If we want to use named tensors with functions that don't handle named tensors, drop the names by renaming them to `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4f249f8-593e-49d8-8153-181a91ee121f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5496,  1.9017,  0.1883,  0.6561, -0.6928],\n",
       "        [-1.0958,  0.1077,  1.0926,  1.0960,  0.0304],\n",
       "        [-0.1555, -1.2855,  0.9975,  0.7204,  0.3737],\n",
       "        [ 0.8061,  1.2827,  1.5501, -1.0633,  1.0668],\n",
       "        [-0.0820,  0.5158,  0.4953,  1.0490,  0.2519]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_named.rename(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd8b9c0-48ea-4847-8010-af91dffba92c",
   "metadata": {},
   "source": [
    "The remainder of this book will use the unnamed version of tensors because this is still experimental"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
